{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gym\n",
      "  Downloading gym-0.17.2.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 81 kB/s eta 0:00:016\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym) (1.18.5)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 30 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle<1.4.0,>=1.2.0\n",
      "  Downloading cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.2-py3-none-any.whl size=1650891 sha256=c9088b2f09cfe188f24bc1a7761f7d5a3318e87329fb05ca7d8c2e86a44324e8\n",
      "  Stored in directory: /home/yuhailin/.cache/pip/wheels/18/e1/58/89a2aa24e6c2cc800204fc02010612afdf200926c4d6bfe315\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, cloudpickle, gym\n",
      "Successfully installed cloudpickle-1.3.0 gym-0.17.2 pyglet-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym,os\n",
    "import  numpy as np\n",
    "import  matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['figure.titlesize'] = 18\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['KaiTi']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "import \ttensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers,optimizers,losses\n",
    "from    PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')  # 创建游戏环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "gamma         = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(keras.Model):\n",
    "    # 策略网络，生成动作的概率分布\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = [] # 存储轨迹\n",
    "        # 输入为状态，输出为左、右2个动作\n",
    "        self.fc1 = layers.Dense(128, kernel_initializer='he_normal')\n",
    "        self.fc2 = layers.Dense(2, kernel_initializer='he_normal')\n",
    "        # 网络优化器\n",
    "        self.optimizer = optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = tf.nn.relu(self.fc1(inputs))\n",
    "        x = tf.nn.softmax(self.fc2(x), axis=1)\n",
    "        return x\n",
    "\n",
    "    def put_data(self, item):\n",
    "        # 记录r,log_P(a|s)\n",
    "        self.data.append(item)\n",
    "\n",
    "    def train_net(self, tape):\n",
    "        # 计算梯度并更新策略网络参数\n",
    "        R = 0 # 初始奖励为0\n",
    "        for r, log_prob in self.data[::-1]:\n",
    "            R = r + gamma * R # 计算每个时间戳上的回报\n",
    "            # 每个时间戳都计算一次梯度\n",
    "            loss = -log_prob * R\n",
    "            with tape.stop_recording():\n",
    "                # 优化策略网络\n",
    "                grads = tape.gradient(loss, self.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.data = [] # 清空轨迹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pi = Policy() # 创建策略网络\n",
    "    pi(tf.random.normal((4,4)))\n",
    "    pi.summary()\n",
    "    score = 0.0 # 计分\n",
    "    print_interval = 20 # 打印间隔\n",
    "    returns = []\n",
    "\n",
    "    for n_epi in range(300):\n",
    "        s = env.reset() # 初始化状态\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            for t in range(501): \n",
    "                # 送入状态向量，获取策略\n",
    "                s = tf.constant(s,dtype=tf.float32)\n",
    "                s = tf.expand_dims(s, axis=0)\n",
    "                prob = pi(s) # 动作分布:[1,2]\n",
    "                # 从类别分布中采样1个动作, shape: [1]\n",
    "                a = tf.random.categorical(tf.math.log(prob), 1)[0]\n",
    "                a = int(a) # Tensor转数字\n",
    "                s_prime, r, done, info = env.step(a)\n",
    "                # 记录动作a和动作产生的奖励r\n",
    "                pi.put_data((r, tf.math.log(prob[0][a])))\n",
    "                s = s_prime # 刷新状态\n",
    "                score += r # 累积奖励\n",
    "\n",
    "                if done:  # 当前episode终止\n",
    "                    break\n",
    "            # episode终止后，训练一次网络\n",
    "            pi.train_net(tape)\n",
    "        del tape\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            returns.append(score/print_interval)\n",
    "            print(f\"# of episode :{n_epi}, avg score : {score/print_interval}\")\n",
    "            score = 0.0\n",
    "    env.close() # 关闭环境\n",
    "\n",
    "    plt.plot(np.arange(len(returns))*print_interval, returns)\n",
    "    plt.plot(np.arange(len(returns))*print_interval, returns, 's')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('total reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"policy_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              multiple                  640       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "# of episode :20, avg score : 15.0\n",
      "# of episode :40, avg score : 16.1\n",
      "# of episode :60, avg score : 18.95\n",
      "# of episode :80, avg score : 32.85\n",
      "# of episode :100, avg score : 34.15\n",
      "# of episode :120, avg score : 36.95\n",
      "# of episode :140, avg score : 60.7\n",
      "# of episode :160, avg score : 65.6\n",
      "# of episode :180, avg score : 84.8\n",
      "# of episode :200, avg score : 118.1\n",
      "# of episode :220, avg score : 136.2\n",
      "# of episode :240, avg score : 201.05\n",
      "# of episode :260, avg score : 327.95\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
