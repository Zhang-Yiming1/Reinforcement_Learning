{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Defining Actions\n",
    "STILL = 0\n",
    "FORWARDS = 1\n",
    "BACKWARDS = -1\n",
    "motion_list = [STILL, FORWARDS, BACKWARDS]\n",
    "\n",
    "NOT_TURN = 0\n",
    "TURN_LEFT = -1\n",
    "TURN_RIGHT = 1\n",
    "turn_list = [NOT_TURN, TURN_LEFT, TURN_RIGHT]\n",
    "\n",
    "GOAL = (5, 6)\n",
    "\n",
    "# Define grid world\n",
    "L = 8\n",
    "W = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = []\n",
    "for x in range(L):\n",
    "    for y in range (W):\n",
    "        for h in range(12):\n",
    "            S.append((x, y, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "for motion in motion_list:\n",
    "    if motion != STILL:\n",
    "        for turn in turn_list:\n",
    "            A.append((motion, turn))\n",
    "    else:\n",
    "        A.append((STILL, NOT_TURN))\n",
    "NA = len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(s):\n",
    "\n",
    "    # Border states (Red, marked X)\n",
    "    if s[0] == 0 or s[0] == L - 1 or s[1] == 0 or s[1] == W - 1:\n",
    "        return -100\n",
    "\n",
    "    # Lane Markers (Yellow, marked --)\n",
    "    elif s[0] == 3 and s[1] in [4, 5, 6]:\n",
    "        return -10\n",
    "\n",
    "    # Goal state (Green, marked *)\n",
    "    elif s[0] == GOAL[0] and s[1] == GOAL[1]:\n",
    "        return 1\n",
    "\n",
    "    # Every other state has reward 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(heading):\n",
    "    dir4 = int((heading + 1) / 3) % 4   # Transform to 4 directions\n",
    "    x = dir4 % 2 if not dir4 % 2 else 2 - dir4\n",
    "    y = (dir4 + 1) % 2 if not (dir4 + 1) % 2 else 1 - dir4\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_prob(heading, pe):\n",
    "    err_headings = []\n",
    "    for turn in turn_list:\n",
    "        # The probability of pre-rotating = \"pe\", and not pre-rotating = \"1 - 2 * pe\"\n",
    "        if turn == STILL:\n",
    "            err_headings.append((heading, 1 - 2 * pe))\n",
    "        else:\n",
    "            err_headings.append(((heading + turn) % 12, pe))\n",
    "    return err_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sa(s, a, s_, pe):\n",
    "    # Pre-rotate probability validity check\n",
    "    if (pe < 0 or pe > 0.5):\n",
    "        return ValueError('Invalid error probability pe. pe should be between 0 and 0.5.')\n",
    "\n",
    "    x, y = s[0], s[1]\n",
    "    prob = 0\n",
    "    # Staying still will result in no pre-rotate error. Future state = Current state.\n",
    "    if a[0] == STILL:\n",
    "        if s_ == s:\n",
    "            prob = 1\n",
    "    # Otherwise, error occurs.\n",
    "    else:\n",
    "        for state in err_prob(s[2], pe):\n",
    "            # Attempting to move off of a grid will result in no linear movement, though rotation portion will still happen.\n",
    "            x = s[0] + a[0] * direction(state[0])[0]\n",
    "            if (0 <= x <= L - 1):\n",
    "                x_p = x\n",
    "            else:\n",
    "                x_p = s[0]\n",
    "\n",
    "            # Consider directions a[0]: FORWARDS (+1) and BACKWARDS (-1)\n",
    "            y = s[1] + a[0] * direction(state[0])[1]\n",
    "            if (0 <= y <= W - 1):\n",
    "                y_p = y\n",
    "            else:\n",
    "                y_p = s[1]\n",
    "\n",
    "            h_p = (state[0] + a[1]) % 12\n",
    "            s_p = (x_p, y_p, h_p)\n",
    "\n",
    "            # Check if the \"s_\" argument is equal to the calculated future states \"s_p\".\n",
    "            if s_ == s_p:\n",
    "                prob = state[1]\n",
    "                return prob\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(s, a, pe):\n",
    "    p={}\n",
    "    for state in S:\n",
    "        if p_sa(s, a, state, pe) > 0:\n",
    "            p[state] = p_sa(s, a, state, pe)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_init(S):\n",
    "\n",
    "    policy = {}\n",
    "\n",
    "    for s in S:\n",
    "        # Get the vector from the state to the goal\n",
    "        dir_vector = [GOAL[0] - s[0], GOAL[1] - s[1]]\n",
    "\n",
    "        # Already reach goal\n",
    "        if dir_vector == [0, 0]:\n",
    "            policy[s] = (STILL, NOT_TURN)\n",
    "            continue\n",
    "\n",
    "        # Compute the moving direction\n",
    "        # Heading +x, which means h in [2, 3, 4]\n",
    "        if s[2] in [2, 3, 4]:\n",
    "            # If target is in the right of robot, go forwards, else go backwards\n",
    "            if dir_vector[0] >= 0:\n",
    "                motion = FORWARDS\n",
    "                dir_vector[0] -= 1\n",
    "            else:\n",
    "                motion = BACKWARDS\n",
    "                dir_vector[0] += 1\n",
    "\n",
    "        # Heading -x, which means h in [8, 9, 10]\n",
    "        if s[2] in [8, 9, 10]:\n",
    "            # If target is in the left of robot, go forwards, else go backwards\n",
    "            if dir_vector[0] <= 0:\n",
    "                motion = FORWARDS\n",
    "                dir_vector[0] += 1\n",
    "            else:\n",
    "                motion = BACKWARDS\n",
    "                dir_vector[0] -= 1\n",
    "\n",
    "        # Heading +y,which means h in [11, 0, 1]\n",
    "        if s[2] in [11, 0, 1]:\n",
    "            # If target is in front of robot, go forwards, else go backwards\n",
    "            if dir_vector[1] >= 0:\n",
    "                motion = FORWARDS\n",
    "                dir_vector[1] -= 1\n",
    "            else:\n",
    "                motion = BACKWARDS\n",
    "                dir_vector[1] += 1\n",
    "\n",
    "        # Heading -y, which means h in [5,6,7]\n",
    "        if s[2] in [5, 6, 7]:\n",
    "            # If target is in front of robot, go backwards, else go forwards\n",
    "            if dir_vector[1] <= 0:\n",
    "                motion = FORWARDS\n",
    "                dir_vector[1] += 1\n",
    "            else:\n",
    "                motion = BACKWARDS\n",
    "                dir_vector[1] -= 1\n",
    "\n",
    "        # Compute the turn direction\n",
    "        # Get the vector angle theta\n",
    "        theta = np.arctan2(dir_vector[1], dir_vector[0]) * 180 / np.pi\n",
    "        angle_diff = s[2] * 30 - (90 - theta)\n",
    "        angle_diff = angle_diff % 180\n",
    "        threshold = 0\n",
    "\n",
    "        # if target is in the left front of robot or right back, turn left\n",
    "        # if they are in one line, not turn\n",
    "        # else turn right \n",
    "        if (angle_diff > threshold) and (angle_diff < 90):\n",
    "            turn = TURN_LEFT\n",
    "        elif angle_diff <= threshold or angle_diff == 90:\n",
    "            turn = NOT_TURN\n",
    "        else:\n",
    "            turn = TURN_RIGHT\n",
    "\n",
    "        policy[s] = (motion, turn)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(policy, s0, pe = 0, show = True):\n",
    "    # policy: a directory of all policy\n",
    "    # s0 = (x, y, h) : initial state\n",
    "    # pe: the error probability pe to pre-rotate when choosing to move.\n",
    "    # Return: Trajectory including passing states and actions on each state\n",
    "\n",
    "    # Confirm the feasibility of pe\n",
    "    if (pe < 0 or pe > 0.5):\n",
    "        return ValueError('Invalid error probability pe. pe should be between 0 and 0.5.')\n",
    "\n",
    "    # Generate the trajectory\n",
    "    trajectory = []\n",
    "    s_now = s0\n",
    "\n",
    "    trajectory.append([s_now, policy[s_now]])\n",
    "\n",
    "    # The robot keeps moving until it reaches target\n",
    "    while (s_now[0] != GOAL[0] or s_now[1] != GOAL[1]):\n",
    "\n",
    "        # Get probability of all possible next states\n",
    "        P_states = next_state(s_now, policy[s_now], pe)\n",
    "        \n",
    "        states = list(P_states.keys())\n",
    "        probs = list(P_states.values())\n",
    "        # Choose next states according to probs\n",
    "        if len(probs) == 1:    \n",
    "            s_next = states[0]\n",
    "            s_now = s_next\n",
    "        else:\n",
    "            # Generate a random float between (0,1)\n",
    "            x = random.random()\n",
    "            if x <= probs[0]:\n",
    "                s_next = states[0]\n",
    "                s_now = s_next\n",
    "            elif probs[0] < x <= (probs[0] + probs[1]):\n",
    "                s_next = states[1]\n",
    "                s_now = s_next\n",
    "            else:\n",
    "                s_next = states[2]\n",
    "                s_now = s_next\n",
    "        # Add state and action now in to trajectory\n",
    "        trajectory.append([s_now, policy[s_now]])\n",
    "\n",
    "    # Grid world initialization\n",
    "    fig = plt.figure(figsize = (L - 2, W - 2))\n",
    "    map = fig.add_subplot(1,1,1)\n",
    "    plt.xlim((0, L))\n",
    "    plt.ylim((0, W))\n",
    "    x_locator = plt.MultipleLocator(1)\n",
    "    y_locator = plt.MultipleLocator(1)\n",
    "    map.xaxis.set_minor_locator(x_locator)\n",
    "    map.yaxis.set_minor_locator(y_locator)\n",
    "    plt.grid(color = 'k', which = 'minor')\n",
    "    plt.tick_params(width = 0)\n",
    "    plt.tick_params(which = 'minor', width = 0)\n",
    "    x_labels = np.arange(0, L, 1)\n",
    "    y_labels = np.arange(0, W, 1)\n",
    "    plt.xticks(x_labels + 0.5, x_labels)\n",
    "    plt.yticks(y_labels + 0.5, y_labels)\n",
    "\n",
    "    # Place red markers\n",
    "    edge1 = plt.Rectangle((0,0), 1, L, color = 'r')\n",
    "    edge2 = plt.Rectangle((0,0), W, 1, color = 'r')\n",
    "    edge3 = plt.Rectangle((L - 1,0), 1, L, color = 'r')\n",
    "    edge4 = plt.Rectangle((0,W - 1), W, 1, color = 'r')\n",
    "    map.add_patch(edge1)\n",
    "    map.add_patch(edge2)\n",
    "    map.add_patch(edge3)\n",
    "    map.add_patch(edge4)\n",
    "\n",
    "    # Place yellow markers\n",
    "    yellow = plt.Rectangle((3, 4), 1, 3, color = 'yellow', alpha = 1)\n",
    "    map.add_patch(yellow)\n",
    "\n",
    "    # Place green goal\n",
    "    goal = plt.Rectangle((5, 6), 1, 1, color = 'greenyellow', alpha = 1)\n",
    "    map.add_patch(goal)\n",
    "\n",
    "    # Plot the start state\n",
    "    plt.plot(s0[0] + 0.5, s0[1] + 0.5, 'o', markersize = '10',color = 'b')\n",
    "    map.arrow(s0[0] + 0.5, s0[1] + 0.5, 0.4 * np.sin(30 * s0[2] * np.pi/180),0.4 * np.cos(30 * s0[2] * np.pi/180), head_width = 0.1, head_length = 0.2, fc = 'k', ec = 'k')\n",
    "\n",
    "    # Plot all passing states\n",
    "    for i in range(0, len(trajectory) - 1):\n",
    "        x1 = trajectory[i][0][0]\n",
    "        y1 = trajectory[i][0][1]\n",
    "        x2 = trajectory[i + 1][0][0]\n",
    "        y2 = trajectory[i + 1][0][1]\n",
    "        h = trajectory[i + 1][0][2]\n",
    "        plt.plot([x1 + 0.5, x2 + 0.5], [y1 + 0.5, y2 + 0.5], 'k--')\n",
    "        plt.plot(x2 + 0.5, y2 + 0.5, 'o', markersize = '10',color = 'b')\n",
    "        map.arrow(x2 + 0.5, y2 + 0.5, 0.4*np.sin(30 * h * np.pi/180), 0.4 * np.cos(30 * h * np.pi/180),\n",
    "                 head_width = 0.1, head_length = 0.2, fc = 'k', ec = 'k')\n",
    "\n",
    "    # Plot the grid world\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value V(s0) is: -1.4636516721127877\n"
     ]
    }
   ],
   "source": [
    "def policy_eval(S, V , policy, reward, pe = 0, discount_factor = 1, threshold = 0.001):\n",
    "\n",
    "    if len(V)==0:\n",
    "        for s in S:\n",
    "            V[s] = 0.0\n",
    "\n",
    "    while True:\n",
    "        diff = 0\n",
    "        # For each state, perform the evaluation\n",
    "        for s in S:\n",
    "            v = 0\n",
    "            # Look at the possible next states:\n",
    "            P_states = next_state(s, policy[s], pe)\n",
    "            states = list(P_states.keys())\n",
    "            probs = list(P_states.values())\n",
    "            # Calculate the value:\n",
    "            for i in range(len(states)):\n",
    "            # Calculate the expected value\n",
    "                s_next = states[i]\n",
    "                v = v + probs[i] * (reward(s_next) + discount_factor * V[s_next])\n",
    "            # Calculate how much the value function changed:\n",
    "            diff = max(diff, np.abs(v - V[s]))\n",
    "            # Update the value function\n",
    "            V[s] = v\n",
    "        # Stop evaluating once the value function change is below a threshold\n",
    "        if diff < threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V = policy_eval(S, {}, policy_init(S), reward, 0, 0.9, 0.01)\n",
    "s0 = (1, 6, 6)\n",
    "print(\"The value V(s0) is:\", V[s0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_one_step_lookahead(S, V, reward, pe = 0, discount_factor = 1):\n",
    "    \n",
    "    policy = {}\n",
    "    \n",
    "    for s in S: \n",
    "        # Initialize all action value as 0.\n",
    "        action = np.zeros(NA)\n",
    "        for i in range(NA):\n",
    "            # Check the possible next state for each action.\n",
    "            P_states = next_state(s, A[i], pe)\n",
    "            states=list(P_states.keys())\n",
    "            probs=list(P_states.values()) \n",
    "            # Calculate value of each action.\n",
    "            for j in range(len(states)):\n",
    "                action[i] += probs[j] * (reward(states[j]) + discount_factor * V[states[j]])\n",
    "\n",
    "       # Get the best action \n",
    "        best_action_index = np.argmax(action)\n",
    "        policy[s] = A[best_action_index]\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(S, V, policy, reward, pe = 0, discount_factor = 1.0):\n",
    "    while True:\n",
    "        # Calculate value of each state\n",
    "        V = policy_eval(S, V, policy, reward, pe, discount_factor, 0.01)\n",
    "        # Update policy by one step lookahead\n",
    "        policy_new = policy_one_step_lookahead(S, V, reward, pe, discount_factor)\n",
    "        # If policy doesn't change, the iteration is done\n",
    "        if policy_new == policy:\n",
    "            return policy, V\n",
    "        else:\n",
    "            policy = policy_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8737050904194734\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy, optimal_V = policy_iteration(S, {}, policy_init(S), reward, 0, 0.9)\n",
    "print(optimal_V[(1, 6, 6)])\n",
    "trajectory = generate_trajectory(optimal_policy, s0 = (1, 6, 6), pe = 0, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(S, reward, pe = 0, discount_factor = 0.9, threshold = 1):\n",
    "    V = {}\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "\n",
    "    while True:\n",
    "        diff = 0\n",
    "        for s in S: \n",
    "        # Initialize all action value as 0.\n",
    "            action = np.zeros(NA)\n",
    "            for i in range(NA):\n",
    "            # Check the possible next state for each action.\n",
    "                P_states = next_state(s, A[i], pe)\n",
    "                states=list(P_states.keys())\n",
    "                probs=list(P_states.values()) \n",
    "            # Calculate value of each action.\n",
    "                for j in range(len(states)):\n",
    "                    action[i] += probs[j] * (reward(states[j]) + discount_factor * V[states[j]])\n",
    "            \n",
    "            # Get the value of the best action\n",
    "            value_best_action = np.max(action)\n",
    "            \n",
    "            # V(H+1) is the best_action value\n",
    "            diff = max(diff, np.abs(value_best_action - V[s]))\n",
    "            \n",
    "            # Update value function\n",
    "            V[s] = value_best_action\n",
    "        if diff < threshold:\n",
    "            break\n",
    "    return V "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
